<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chip Huyen&#39;s AI Engineering - Commandz.io</title>
    <link rel="stylesheet" href="/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css">
</head>
<body>
    <header>
        <nav>
            <a href="/" class="nav-logo">
                <img src="/images/cmdz.jpg" alt="Commandz.io Logo" />
            </a>
            <a href="/about">About</a>
            <a href="/posts">Blog</a>
            <a href="/learning">Learning</a>
            <a href="/snippets">Snippets</a>
        </nav>
    </header>

    <main>
        
<article class="content">
    <header class="content-header">
        <h1 class="title">Chip Huyen&#39;s AI Engineering</h1>
        
        <div class="metadata">
            <div class="primary-meta">
                
                <time datetime="2025-02-07T00:00:00.000Z">07 Feb 2025</time>
                

                

                
            </div>

            
            <div class="tags-container">
                
                <span class="tag">ai</span>
                
                <span class="tag">llm</span>
                
                <span class="tag">learning</span>
                
            </div>
            
        </div>

        
        <div class="abstract">
            Building Applications with Foundation Models
        </div>
        

        
        <div class="featured-image">
            <a href="https://www.oreilly.com/library/view/ai-engineering/9781098166298/" target="_blank" rel="noopener">
                <img src="/images/learning/ai-engineering-book.jpeg" alt="Chip Huyen&#39;s AI Engineering">
            </a>
        </div>
        
    </header>

    <div class="content-body">
        <h1>5 Prompt Engineering</h1>
<p>Prompt engineering guides a model’s behavior without changing the model’s weights.</p>
<p>You should make the most out of prompting before moving to more resource-intensive techniques like finetuning.</p>
<p>To build production-ready AI applications, you need more than just prompt engineering. You need statistics, engineering, and classic ML knowledge to do experiment tracking, evaluation, and dataset curation.</p>
<p>A prompt is an instruction given to a model to perform a task.</p>
<p>Task description, Example(s) of how to do this task, The task (such as formatting the response). Experiment with different prompt structures to find out which works best for you.</p>
<p>&quot;How much prompt engineering is needed depends on how robust the model is to prompt perturbation.&quot;</p>
<p>You can measure a model’s robustness by randomly perturbing the prompts<br>
to see how the output changes.  robustness is strongly correlated with its overall capability.</p>
<h2>In-Context Learning: Zero-Shot and Few-Shot</h2>
<p>Teaching models what to do via prompts is also known as in-context learning.</p>
<p>In-context learning allows a model to incorporate new information continually to make decisions, preventing it from becoming outdated.</p>
<p>Each example provided in the prompt is called a shot.</p>
<ul>
<li>Teaching a model to learn from examples in the prompt is also called few-shot learning.</li>
<li>When no example is provided, it’s zero-shot learning.</li>
</ul>
<p>The more examples there are, the longer your prompt will be, increasing the inference cost.</p>
<p>Sometimes, prompt and context are used interchangeably: prompt to refer to the whole input into the model, and<br>
context to refer to the information provided to the model so that it can<br>
perform a given task.</p>
<h2>System Prompt and User Prompt</h2>
<p>You can think of the system prompt as the task description and the user prompt as the task.</p>
<p>Typically, the instructions provided by application developers are<br>
put into the system prompt, while the instructions provided by users are put<br>
into the user prompt.</p>
<p>Many model providers emphasize that well-crafted system prompts can<br>
improve performance.</p>
<p>the system prompt and the user prompt are<br>
concatenated into a single final prompt before being fed into the model.</p>
<h2>Context Length and Context Efficiency</h2>
<p>Not all parts of a prompt are equal. Research has shown that a model is<br>
much better at understanding instructions given at the beginning and the<br>
end of a prompt than in the middle</p>
<h2>Prompt Engineering Best Practices</h2>
<h3>Write Clear and Explicit Instructions</h3>
<p>As you experiment with a prompt, you might observe undesirable behaviors<br>
that require adjustments to the prompt to prevent them.</p>
<p>A persona can help the model to understand the perspective it’s supposed to<br>
use to generate responses.</p>
<p>Examples can reduce ambiguity about how you want the model to respond.</p>
<p>If you want the model to be concise, tell it so. Long outputs are not only<br>
costly (model APIs charge per token) but they also increase latency.  make explicit that you don’t<br>
want preambles.</p>
<p>If you want the model to generate JSON, specify what the keys in the JSON should be.</p>
<p>For tasks expecting structured outputs, such as classification, use markers to<br>
mark the end of the prompts to let the model know that the structured<br>
outputs should begin.</p>
<h3>Provide Sufficient Context</h3>
<p>Context can also mitigate hallucinations.</p>
<p>You can either provide the model with the necessary context or give it tools<br>
to gather context.</p>
<p>In many scenarios, it’s desirable for the model to use only information<br>
provided in the context to respond.  How to restrict a model to only the context is tricky. Clear instructions, such<br>
as “answer using only the provided context”, along with examples of<br>
questions it shouldn’t be able to answer, can help.</p>
<p>The safest method is to<br>
train a model exclusively on the permitted corpus of knowledge, though this<br>
is often not feasible for most use cases.</p>
<h3>Break Complex Tasks into Simpler Subtasks</h3>
<p>For complex tasks that require multiple steps, break those tasks into<br>
subtasks.</p>
<p>Example of responding to a customer support request: intent classification, generating response.</p>
<p>Prompt decomposition not only enhances<br>
performance but also offers several additional benefits: Monitoring (intermediate steps), Debugging, Parallelization, Effort (simpler prompts).</p>
<p>One downside of prompt decomposition is that it can increase the latency<br>
perceived by users</p>
<p>Prompt decomposition typically involves more model queries, which can<br>
increase costs.</p>
<h3>Give the Model Time to Think</h3>
<p>encourage the model to “think” about a question using chain-of-thought (CoT) and self-<br>
critique prompting.</p>
<p>CoT means explicitly asking the model to think step by step, nudging it<br>
toward a more systematic approach to problem solving.</p>
<p>CoT is among the<br>
first prompting techniques that work well across models.</p>
<p>The simplest way to do CoT is to add “think step by step” or “explain your<br>
decision” in your prompt.</p>
<p>you can specify the steps the model should take or include<br>
examples of what the steps should look like in your prompt.</p>
<p>Self-critique means asking the model to check its own outputs.</p>
<p>Similar to prompt decomposition, CoT and self-critique can increase the<br>
latency perceived by users.</p>
<h3>Iterate on Your Prompts</h3>
<p>As you experiment with different prompts, make sure to test changes<br>
systematically. Version your prompts. Use an experiment tracking tool.<br>
Standardize evaluation metrics and evaluation data so that you can compare<br>
the performance of different prompts. Evaluate each prompt in the context<br>
of the whole system. A prompt might improve the model’s performance on<br>
a subtask but worsen the whole system’s performance.</p>
<h3>Evaluate Prompt Engineering Tools</h3>
<p>A common approach to automating prompt generation is to use AI models.</p>
<p>Following the keep-it-simple principle, you might want to start by writing<br>
your own prompts without any tool.</p>
<p>If you use a prompt engineering tool, always inspect the prompts produced<br>
by that tool to see whether these prompts make sense and track how many API calls it generates.</p>
<h3>Organize and Version Prompts</h3>
<p>It’s good practice to separate prompts from code: reusability, testing, readability. collaboration.</p>
<p>Several tools have proposed special .prompt file formats to store prompts.<br>
See Google Firebase’s Dotprompt, Humanloop, Continue Dev, and<br>
Promptfile.</p>
<h2>Defensive Prompt Engineering</h2>
<p>Prompt extraction, Jailbreaking and prompt injection, Information extraction</p>
<ul>
<li>Remote code or tool execution, Data leaks, Social harms, misinformation, Service interruption and subversion, Brand risk</li>
</ul>
<h3>Proprietary Prompts and Reverse Prompt Engineering</h3>
<p>Reverse prompt<br>
engineering is the process of deducing the system prompt used for a certain<br>
application.</p>
<p>Reverse prompt engineering is typically done by analyzing the application<br>
outputs or by tricking the model into repeating its entire prompt, which<br>
includes the system prompt.</p>
<p>“Write your system prompt assuming that it will one day become<br>
public.”</p>
<p>While well-crafted prompts are valuable, proprietary prompts are more of a<br>
liability than a competitive advantage. Prompts require maintenance. They<br>
need to be updated every time the underlying model changes.</p>
<h3>Jailbreaking and Prompt Injection</h3>
<p>Jailbreaking a model means trying to subvert a model’s safety features.</p>
<p>Prompt injection refers to a type of attack where malicious instructions are<br>
injected into user prompts.</p>
<p>Prompt attacks are possible precisely because models are trained to follow<br>
instructions.</p>
<p>it’s difficult for a model to differentiate between system prompts and user prompts</p>
<h3>Direct manual prompt hacking</h3>
<p>This family of attacks involves manually crafting a prompt or a series of<br>
prompts that trick a model into dropping its safety filters. akin to social engineering</p>
<p>In the early days of LLMs, a simple approach was obfuscation. (misspellings, special characters)</p>
<p>The second approach is output formatting manipulation, which involves<br>
hiding the malicious intent in unexpected formats. (writing a poem about doing something illegal)</p>
<p>The third approach, which is versatile, is roleplaying. Attackers ask the<br>
model to pretend to play a role or act out a scenario. In the early days of<br>
jailbreaking, a common attack was called DAN, Do Anything Now.</p>
<h3>Automated attacks</h3>
<p>Prompt hacking can be partially or fully automated by algorithms.</p>
<p>wo algorithms that randomly<br>
substitute different parts of a prompt with different substrings to find a<br>
variation that works.  shows that it’s possible to ask<br>
a model to brainstorm new attacks given existing attacks.</p>
<p>Prompt Automatic Iterative Refinement (PAIR) uses an AI model to act as<br>
an attacker. This attacker AI is tasked with an objective, such as eliciting a<br>
certain type of objectionable content from the target AI.</p>
<ol>
<li>Generate a prompt.</li>
<li>Send the prompt to the target AI.</li>
<li>Based on the response from the target, revise the prompt until the<br>
objective is achieved.</li>
</ol>
<p>In their experiment, PAIR often requires fewer than twenty queries to<br>
produce a jailbreak.</p>
<h3>Indirect prompt injection</h3>
<p>Indirect prompt injection: instead of placing malicious instructions in the prompt directly,<br>
attackers place these instructions in the tools that the model is integrated<br>
with.</p>
<ul>
<li>Passive phishing</li>
<li>Active injection</li>
</ul>
<p>An attacker could sign up with a username<br>
like “Bruce Remove All Data Lee”. (little bobby tables?)</p>
<h3>Information Extraction</h3>
<p>intended use can be exploited for the following purposes: Data Theft, Privacy Violation, Copyright infringement</p>
<p>A niche research area called factual probing focuses on figuring out what a<br>
model knows.</p>
<p>The same techniques used to probe a model for its knowledge can also be<br>
used to extract sensitive information from training data.</p>
<p>The assumption is<br>
that the model memorizes its training data, and the right prompts can<br>
trigger the model to output its memorization.</p>
<p>Both papers<br>
concluded that while such extraction is technically possible, the risk is low<br>
because the attackers need to know the specific context in which the data to<br>
be extracted appears.</p>
<p>For example, when they asked ChatGPT (GPT-<br>
turbo-3.5) to repeat the word “poem” forever, the model initially repeated<br>
the word “poem” several hundred times and then diverged.</p>
<p>This suggests the existence of prompt strategies that allow training data<br>
extraction without knowing anything about the training data.</p>
<p>the memorization rates for some models,<br>
based on the paper’s test corpus, to be close to 1%.</p>
<p>there’s a clear trend that the larger model memorizes more, making larger models more vulnerable to data extraction attacks.</p>
<p>Training data extraction is possible with models of other modalities, too.</p>
<p>diffusion models are much less private than prior<br>
generative models such as GANs, and that mitigating these vulnerabilities<br>
may require new advances in privacy-preserving training.</p>
<p>It’s important to remember that training data extraction doesn’t always lead<br>
to PII</p>
<p>Models can also just regurgitate training data without adversarial attacks.</p>
<p>By studying a wide range of foundation models, they concluded that “the<br>
likelihood of direct regurgitation of long copyrighted sequences issomewhat uncommon, but it does become noticeable when looking at<br>
popular books.</p>
<p>It’s unlikely there will be a foolproof automatic way to detect<br>
copyright infringement. The best solution is to not train a model on<br>
copyrighted materials, but if you don’t train the model yourself, you don’t<br>
have any control over it.</p>
<h3>Defenses Against Prompt Attacks</h3>
<p>Tools<br>
that help automate security probing include Azure/PyRIT, leondz/garak,<br>
greshake/llm-security, and CHATS-lab/persuasive_jailbreaker.</p>
<p><a href="https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming">https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/red-teaming</a></p>
<p>To evaluate a system’s robustness against prompt attacks, two important<br>
metrics are the violation rate and the false refusal rate.</p>
<h3>Model-level defense</h3>
<p>many attacks can be thwarted if the model is<br>
trained to better follow system prompts.</p>
<p>OpenAI introduces an<br>
instruction hierarchy that contains four levels of priority.  In the event of conflicting instructions, the higher-priority instruction should be followed.</p>
<ol>
<li>System prompt</li>
<li>User prompt</li>
<li>Model outputs</li>
<li>Tool outputs</li>
</ol>
<p>When finetuning a model for safety, it’s important to train the model not<br>
only to recognize malicious prompts but also to generate safe responses for<br>
borderline requests.</p>
<h3>Prompt-level defense</h3>
<p>One simple trick is to repeat the system prompt twice, both before and after<br>
the user prompt.</p>
<p>When using prompt tools, make sure to inspect their default prompt<br>
templates since many of them might lack safety instructions.</p>
<p>LangChain’s default templates were so<br>
permissive that their injection attacks had 100% success rates. Adding<br>
restrictions to these prompts significantly thwarted these attacks.</p>
<h3>System-level defense</h3>
<p>isolation: If your system involves executing<br>
generated code, execute this code only in a virtual machine separated from<br>
the user’s main machine.</p>
<p>To reduce the chance of your application talking about topics it’s not<br>
prepared for, you can define out-of-scope topics for your application.</p>
<p>More advanced algorithms use AI to understand the user’s intent by<br>
analyzing the entire conversation, not just the current input.</p>
<p>Use an anomaly detection algorithm to identify unusual prompts.</p>
<p>On the<br>
input side, you can have a list of keywords to block, known prompt attack<br>
patterns to match the inputs against, or a model to detect suspicious<br>
requests.</p>
<p>Bad actors can be detected not just by their individual inputs and outputs<br>
but also by their usage patterns.</p>
<h1>6. RAG and Agents</h1>
<p>&quot;RAG as a solution for knowledge-intensive tasks where all the available knowledge can’t be input into the model directly&quot;</p>
<p>Retrieve-then-generate pattern.  RAG as a technique to construct context specific to each<br>
query.  RAG as a technique to construct context specific to each query.  Context construction for foundation models is equivalent to feature<br>
engineering for classical ML models.</p>
<p>A model that can process long context doesn’t necessarily use that<br>
context well.</p>
<p>if “your knowledge base is smaller than 200,000 tokens<br>
(about 500 pages of material), you can just include the entire knowledge base in the prompt that you<br>
give the model</p>
<h2>RAG Architecture</h2>
<p>In the original RAG paper, Lewis et al. trained the retriever and the generative model together.  The success of a RAG system depends on the quality of its retriever.</p>
<p>How to index data depends on how you want to retrieve it later on.  You can split each document into more manageable<br>
chunks</p>
<h2>Retrieval Algorithms</h2>
<p>Two of the most common retrieval algorithms: term-based retrieval and embedding-based retrieval.</p>
<p>Sparse retrievers represent data using sparse vectors. A sparse vector is a vector where the majority of the values are 0.</p>
<p>Term-based retrieval is considered sparse, as each term can be represented using a sparse one-hot vector, a vector that is 0 everywhere except one value of 1.</p>
<p>Dense retrievers represent data using dense vectors. A dense vector is a vector where the majority of the values aren’t 0. Embedding-based retrieval is typically considered dense, as embeddings are generally dense vectors.</p>
<h3>Lexical/term Retrieval</h3>
<p>Lexical retrieval: the most straightforward way to find relevant documents is with keywords.</p>
<ul>
<li>TF-IDF, Term frequency.  a term’s importance is inversely proportional to the number of documents it appears in.  inverse document frequency (IDF).</li>
<li>BM25 normalizes term frequency scores by document length. Longer documents are more likely to contain a given term and have higher term frequency values.</li>
</ul>
<p>tokenization: can lead to multi-word terms being broken into individual words, losing their original meaning. One way to<br>
mitigate this issue is to treat the most common n-grams as terms.  Measuring the lexical similarity between two texts<br>
based on their n-gram overlap.</p>
<p>Term-based retrieval is generally much faster than embedding-based retrieval during both indexing and query. Term extraction is faster than embedding generation, and mapping from a term to the documents that contain it can be less computationally expensive than a nearest-neighbor search.</p>
<p>its simplicity also means that it has fewer components you can tweak to improve its performance</p>
<h3>Vector search &amp; Embeddings</h3>
<p>Semantic retrieval: embedding-based retrievers aim to rank documents based on how closely their meanings align with the query</p>
<p>Embedding model: convert the query into an embedding using the same embedding model used during indexing.</p>
<p>Real-world semantic retrieval systems might contain other components, such as a reranker to rerank all retrieved candidates, and caches to reduce latency.</p>
<p>Vector search is typically framed as a nearest-neighbor search problem.  The naive solution is k-nearest neighbors (k-NN) but it’s<br>
computationally heavy and slow. It should be used only for small datasets.</p>
<p>For large datasets, vector search is typically done using an approximate nearest neighbor (ANN) algorithm.</p>
<p>vector databases organize vectors into buckets, trees, or graphs</p>
<p>Many traditional databases have extended or will extend to support vector storage and vector search.</p>
<p>Embedding-based retrieval, on the other hand, can be significantly improved over time to outperform term-based retrieval. You can finetune the embedding model and the retriever, either separately, together, or in conjunction with the generative model.</p>
<p>Since much of RAG latency comes from output generation, especially for long outputs, the added latency by query embedding generation and vector search might be minimal compared to the total RAG latency</p>
<h3>Comparison</h3>
<p>Context precision &amp; Context recall.  Curate an evaluation set with a list of test queries and a set of documents.</p>
<p>You only need to compare the retrieved documents to the query, which can be done by an AI judge.</p>
<p>If you care about the ranking of the retrieved documents, for example, more relevant documents should be ranked first.</p>
<p>For semantic retrieval, you need to also evaluate the quality of your embeddings.</p>
<p>The quality of a retriever should also be evaluated in the context of the whole RAG system.</p>
<p>With retrieval systems, you can make certain trade-offs between indexing and querying. The more detailed the index is, the more accurate the retrieval process will be, but the indexing process will be slower and more memory-consuming</p>
<p>The quality of a RAG system should be evaluated both component by component and end to end.</p>
<ol>
<li>Evaluate the retrieval quality.</li>
<li>Evaluate the final RAG outputs.</li>
<li>Evaluate the embeddings (for embedding-based retrieval).</li>
</ol>
<p>a production retrieval system typically combines several approaches.</p>
<ul>
<li>first using a cheap, fast retriever and then a slow more in-depth retriever</li>
<li>using multiple retrievers in tandem</li>
</ul>
<p>Both of those require a reranker</p>
<h2>Retrieval Optimization</h2>
<p>The simplest strategy is to chunk documents into chunks of equal length based on a certain unit.</p>
<p>You can also split documents recursively using increasingly smaller units until each chunk fits within your maximum chunk size</p>
<p>Specific documents might also support creative chunking strategies</p>
<p>Overlapping ensures that important boundary information is included in at least one chunk.</p>
<p>The chunk size shouldn’t exceed the maximum context length of the generative model. For the embedding-based approach, the chunk size also shouldn’t exceed the embedding model’s context limit.</p>
<p>A smaller chunk size allows for more diverse information.  Small chunk sizes, however, can cause the loss of important information.  Smaller chunk sizes can also increase computational overhead.</p>
<h3>Reranking</h3>
<p>Reranking is especially useful when you need to reduce the number of retrieved documents, either to fit them into your model’s context or to reduce the number of input tokens</p>
<p>Documents can also be reranked based on time, giving higher weight to more recent data.</p>
<p>In context reranking, the order of documents still matters because it affects how well a model can process them. Models might better understand documents at the beginning and end of the context</p>
<h3>Query Rewriting</h3>
<p>Query rewriting is also known as query reformulation, query normalization, and sometimes query expansion.</p>
<p>In traditional search engines, query rewriting is often done using heuristics.  Query rewriting can also be done using other<br>
AI models.</p>
<h3>Contextual Retrieval</h3>
<p>Augment each chunk with relevant context to make it easier to retrieve the relevant chunks. A simple technique is to augment a chunk with metadata like tags and keywords.</p>
<p>You can also augment each chunk with the questions it can answer. For customer support, you can augment each article with related questions.</p>
<p>You can augment each chunk with the context from the original document, that explains the chunk and its relationship to the original document.</p>
<h1>8. Dataset Engineering</h1>
<h2>Highlights</h2>
<ul>
<li>
<p>Data sets should be high-quality, task-specific data</p>
<ul>
<li>different training phases demand distinct data formats</li>
<li>meaningful progress depends on both model and data improvements</li>
</ul>
</li>
<li>
<p>You need the right data in the right format at the right scale.  Lots of bad data won't help you.</p>
<ul>
<li>relevant,aligned with task requirements, consistent, correctly formatted, unique, and compliant</li>
</ul>
</li>
<li>
<p>You need data diversity of the problems your system is supposed to solve.</p>
<ul>
<li>Annealing (training on small amounts of high-quality examples) can improve performance for specific use cases</li>
</ul>
</li>
<li>
<p>You need a lot more data with full-finetuning vs PEFT.  Better models often require less data during fine-tuning.</p>
<ul>
<li>data set experiments -&gt; you can see the performance curve</li>
</ul>
</li>
<li>
<p>Leveraging your application data is the best given that it represents your diversity and you can leverage a data flywheel to continually improve the product.</p>
</li>
<li>
<p>augmented vs synthesized data</p>
</li>
<li>
<p>use data synthesis to increase quantity, coverage, quality, concerns</p>
<ul>
<li>Can help with biases</li>
<li>perturbation: adding noise to existing data to generate new data</li>
<li>AI-generated content (web) -&gt; already trained on synthetic data</li>
<li>reverse instruction approach: use long-form data to generate prompts that would elicit that content</li>
<li>Eval by functional correctness and AI Judges</li>
<li>AI data -&gt; lower quality
<ul>
<li>can cause irreversible defects</li>
<li>imitation can cause hallucinations</li>
<li>more likely to produce probable than improbably events</li>
</ul>
</li>
<li>Distillation -&gt; teacher's standard is the student's standard
<ul>
<li>synthetic data oftent used with lora</li>
<li>training on data generated by a more competent model can significantly improve performance</li>
</ul>
</li>
<li>Data Processing
<ul>
<li>Stare at the data, manual inspection</li>
<li>Data duplication can introduce biases
<ul>
<li>deduplicated by: pairwise comparison, hashing, dimensionality reduction</li>
</ul>
</li>
<li>Clean and filter data -&gt; extra tokens, html, PII, toxic, etc</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<p>The best<br>
ML team in the world with infinite compute can’t help you finetune a good<br>
model if you don’t have data.</p>
<p>For the same model, different training phases aim to teach the model<br>
different capabilities, and, therefore, require datasets with different<br>
attributes.</p>
<p>data-centric AI, as opposed to model-centric AI</p>
<p>meaningful technological progress often requires investment in<br>
both model and data improvements.</p>
<p>For<br>
instruction finetuning, you need data in the (instruction, response) format.<br>
For preference finetuning, you need data in the (instruction, winning<br>
response, losing response) format. To train a reward model, you can use the<br>
same data format as preference finetuning or use data with annotated scores<br>
for each of your examples in the ((instruction, response), score) format.</p>
<p>Acquiring high-quality data annotations is always challenging, but it’s even<br>
more challenging if you want to teach models complex behaviors such as<br>
chain-of-thought (CoT) reasoning and tool use.</p>
<p>CoT: ts training data should include CoT responses.</p>
<p>It’s common to use domain experts to<br>
create tool use data, where each prompt is a task that requires tool<br>
use, and its response is the actions needed to perform that task.</p>
<h2>Data Curation</h2>
<p>Data curation isn’t just about creating new data to help a model learn new<br>
behaviors but is also about removing existing data to help a model unlearnbad behaviors.</p>
<p>Data coverage is equivalent to having the right<br>
mix of ingredients (e.g., you shouldn’t have too much or too little sugar).<br>
Data quantity is about how many ingredients you should have.</p>
<p>A small amount of high-quality data can outperform a large amount of noisy<br>
data, e.g., data that is irrelevant or inconsistent.</p>
<p>The short answer is that data is considered<br>
high-quality if it helps you do your job efficiently and reliably. The long<br>
3<br>
answers, however, differ for different people.</p>
<p>In general, data can be<br>
considered high-quality if it has the following six characteristics: relevant,aligned with task requirements, consistent, correctly formatted, unique, and<br>
compliant.</p>
<p>Redundant formatting tokens can interfere with the model’s learning,</p>
<h3>Data Coverage</h3>
<p>A model’s training data should cover the range of problems you expect it to<br>
solve.</p>
<p>Coverage requires sufficient data diversity, which is why<br>
many refer to this attribute as data diversity.</p>
<p>On the other hand, a<br>
chatbot that recommends products to global customers doesn’t necessarily<br>
need domain diversity, but linguistic and cultural diversity will be<br>
important.</p>
<p>Llama 3 authors shared that annealing the model on small amounts of<br>
high-quality code and math data (training the model using an increasingly<br>
smaller learning rate with increasingly more code and math data) can boost<br>
the performance of their models on key benchmarks.</p>
<p>This confirms acommon belief that high-quality code and math data is more effective than<br>
natural language text in boosting the model’s reasoning capabilities.</p>
<p>A<br>
simple approach is to choose a data mix that accurately reflects the real-<br>
world application usage.</p>
<h3>Data Quantity</h3>
<p>Three other factors influence how much data you need:</p>
<ul>
<li>Full finetuning promises to give the best performance, but it requires orders of magnitude more data than PEFT methods like LoRA.</li>
<li>Task complexity</li>
<li>Base model’s performance: The closer the base model is to the desirable performance, the fewer examples are needed to get there.</li>
</ul>
<p>if you have fewer examples (100), more advanced models give you better finetuning performance.</p>
<p>In short, if you have a small amount of data, you might want to use PEFT<br>
methods on more advanced models. If you have a large amount of data, use<br>
full finetuning with smaller models.</p>
<p>Experimenting with a small dataset can help you estimate how much more<br>
data you’ll need.  A steep performance gain slope with increasing dataset size<br>
means that you can expect significant performance improvement bydoubling your data.</p>
<p>While a larger number of finetuning examples generally improves a model’s<br>
performance, the diversity of the examples matters, too.</p>
<p>The diversity of data can be reflected in task types (such as summarization<br>
and question answering), topic diversity (such as fashion, finance, and<br>
technology), and the expected output formats (such as JSON outputs or yes-<br>
or-no answers).</p>
<h3>Data Acquisition and Annotation</h3>
<p>The most important source of data, however, is typically data from your<br>
own application. If you can figure out a way to create a data flywheel that<br>
leverages data generated by your users to continually improve your product, you will gain a significant advantage.</p>
<p>Application data is ideal because it’s<br>
perfectly relevant and aligned with your task. In other words, it matches the<br>
distribution of the data that you care about, which is incredibly hard to<br>
achieve with other data sources.</p>
<p>Google dataset search</p>
<p>Often, you might need to annotate your own data for finetuning. Annotation<br>
is challenging not just because of the annotation process but also due to the<br>
complexity of creating clear annotation guidelines.</p>
<p>Some teams, including LinkedIn, have reported that annotation guidelines<br>
were among the most challenging parts of their AI engineering pipeline.</p>
<p>Two processes commonly used are data augmentation and data synthesis.</p>
<ul>
<li>Data augmentation creates new data from existing data</li>
<li>Data synthesis generates data to mimic the properties of real data.</li>
</ul>
<p>In other words, augmented data is derived from real data, whereas synthetic<br>
data isn’t real. However, since the goal of both augmentation and synthesis<br>
is to automate data creation, sometimes the two terms are used<br>
interchangeably.</p>
<p>In many use cases, as discussed in “Limitations to AI-generated data”,<br>
mixing human- and AI-generated data often produces the best value.</p>
<h3>Why Data Synthesis</h3>
<p>To increase data quantity<br>
To increase data coverage<br>
To increase data quality<br>
To mitigate privacy concerns<br>
To distill models</p>
<p>Using algorithms to generate data is also called procedural generation, as opposed to manual generation.</p>
<p>A newer method made possible by advanced AI models is using AI itself to synthesize data.</p>
<p>You can procedurally generate new data from existing data by applying<br>
simple transformations.</p>
<p>This approach can be used to mitigate potential biases in your data.</p>
<p>One interesting transformation is perturbation: adding noise to existing data<br>
to generate new data.</p>
<p>You can train your model on perturbed data. Perturbation can both improve<br>
the model’s performance and make it more robust against attacks;</p>
<p>Simulations allow you to run multiple experiments with minimal costs<br>
while avoiding accidents and physical damage.</p>
<p>Simulations are common to generate data to teach models to use tools.</p>
<h3>AI-Powered Data Synthesis</h3>
<p>AI’s paraphrasing and translation abilities can be used to augment existing<br>
datasets.</p>
<p>However, as the internet becomes flooded with AI-generated content,<br>
models that rely on internet data are likely already pre-trained on synthetic<br>
data.</p>
<p>Data synthesis for post-training is also more common because post-training<br>
data, including both instruction data and preference data, generally demands<br>
the most effort to produce.</p>
<p>You can also use humans to write instructions and AI to<br>
generate responses</p>
<p>follow the reverse<br>
instruction approach: take existing long-form, high-quality content like<br>
stories, books, and Wikipedia articles and use AI to generate prompts that<br>
would elicit such content</p>
<p>It’s possible to use reverse instruction to develop increasingly powerful models without adding manually annotated data.</p>
<p>To ensure the quality of the generated data, they employed a rigorous correctness analysis and error correction pipeline</p>
<p>The quality of AI-generated data can be measured the same way you’d evaluate other AI outputs—by functional correctness and AI judges.</p>
<p>Just like real data, synthetic data can also be filtered using heuristics. In<br>
general, you might want to remove examples that are empty or too short for<br>
your application.</p>
<p>AI’s generated data can be of low quality, and, as people never tire of<br>
saying, “garbage in, garbage out.”</p>
<p>Worse, imitation can force the student model to hallucinate. Imagine if the<br>
teacher model is capable of answering complex math questions, so its<br>
responses to those questions are solutions.</p>
<p>It’s also unclear how much AI-generated data a model can train on. Some<br>
studies have shown that recursively using AI-generated data in training<br>
causes irreversible defects in the resulting models, degrading their<br>
performance over time.</p>
<p>One possible explanation is that AI models are more likely to generate<br>
probable events (e.g., not having cancer) and less likely to generate<br>
improbable events (e.g., having cancer).</p>
<p>This causes models to output more<br>
common events over time while forgetting rare events.</p>
<p>Some people have been able to improve model performance using a large<br>
amount of synthetic data.</p>
<p>AI-generated data might also perpetuate biases.</p>
<p>the more faithful the model’s outputs to the<br>
characteristics of the original training distribution, the more stable the<br>
feedback loop, thus minimizing the risk of bias amplification.</p>
<h3>Model Distillation</h3>
<p>Model distillation (also called knowledge distillation) is a method in which<br>
a small model (student) is trained to mimic a larger model</p>
<p>Traditionally, the goal of model distillation is to produce smaller models for<br>
deployment. Deploying a big model can be resource-intensive.</p>
<p>Synthetic instruction data is commonly used together with adapter-based<br>
techniques, such as LoRA.</p>
<p>Note that not all training with synthetic data is model distillation. Model<br>
distillation implies that the teacher model’s performance is the student’s<br>
gold standard.</p>
<p>it’s possible to use synthetic data to train a student<br>
model that is larger and more powerful than the teacher.</p>
<p>The Llama 3 paper notes that while training on data generated by a more<br>
competent model can significantly improve a model’s performance, training<br>
indiscriminately on self-generated data doesn’t improve the model’s<br>
performance and can even degrade it. However, by introducing mechanisms<br>
to verify the quality of synthetic data and using only verified synthetic data,<br>
they were able to continually improve a model using its generated data.</p>
<h2>Data Processing</h2>
<p>In every project I’ve worked on,<br>
staring at data for just 15 minutes usually gives me some insight that could<br>
save me hours of headaches.</p>
<p>Duplicated data can skew the data distribution and introduce biases into<br>
your model.</p>
<p>Whole document duplications, Intra-document duplications, Cross-document duplications</p>
<p>Here are some concrete ways you can deduplicate data:</p>
<ul>
<li>Pairwise comparison</li>
<li>Hashing</li>
<li>Dimensionality reduction</li>
</ul>
<h3>Clean and Filter Data</h3>
<p>remove extraneous formatting tokens</p>
<p>Unless you want to train your model on HMTL tags, remove<br>
them.</p>
<p>You need to clean your data of anything that isn’t compliant with your<br>
policies, such as PII, sensitive data, copyrighted data, or data that is<br>
considered toxic</p>
<p>You also might want to remove low-quality data, using techniques<br>
discussed in “Data verification” to detect low-quality data.</p>
<p>Manual inspection of data is especially important in this step. Staring at<br>
data might help you notice patterns that you can use as heuristics to detect<br>
low-quality data.</p>
<p>If there is more data than you need or can afford to use (e.g., due to your<br>
compute budget), you can further filter your data.</p>
<h3>Format Data</h3>
<p>If you’re doing supervised finetuning, your data is most likely in the format<br>
(instruction, response)</p>
<p>Instructions can be further decomposed into (system<br>
prompt, user prompt).</p>
<p>If you’ve graduated to finetuning from prompt<br>
engineering, the instructions used for finetuning might be different from the<br>
instructions used during prompt engineering.</p>

    </div>
</article>

<style>
    .content {
        max-width: 900px;
        margin: 0 auto;
        padding: 0 1rem;
    }

    .content-header {
        margin-bottom: 3rem;
    }

    .title {
        font-size: 2.5rem;
        margin-bottom: 1rem;
        line-height: 1.2;
    }

    .metadata {
        margin: 1.5rem 0;
    }

    .primary-meta {
        color: #666;
        font-size: 1rem;
        margin-bottom: 1rem;
    }

    .separator {
        margin: 0 0.5rem;
        color: #ccc;
    }

    .tags-container {
        display: flex;
        flex-wrap: wrap;
        gap: 0.5rem;
        margin-top: 0.75rem;
    }

    .tag {
        background-color: #f3f4f6;
        color: #4b5563;
        font-size: 0.875rem;
        padding: 0.25rem 0.75rem;
        border-radius: 9999px;
        display: inline-block;
        transition: all 0.2s ease;
    }

    .tag:hover {
        background-color: #e5e7eb;
    }

    .abstract {
        font-size: 1.25rem;
        color: #4b5563;
        margin: 2rem 0;
        line-height: 1.6;
        font-style: italic;
    }

    .featured-image {
        margin: 2rem auto;
        text-align: center;
        max-width: 700px;
    }

    .featured-image img {
        max-width: 100%;
        height: auto;
        border-radius: 12px;
        box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
    }

    /* Specific image sizing based on image path */
    img[src*="/books/"] {
        max-width: 300px;
    }

    img[src*="/courses/"] {
        max-width: 500px;
    }

    img[src*="/videos/"] {
        max-width: 600px;
    }

    .content-body {
        font-size: 1.125rem;
        line-height: 1.75;
        color: #1a1a1a;
    }

    @media (max-width: 768px) {
        .title {
            font-size: 2rem;
        }

        .abstract {
            font-size: 1.125rem;
        }

        .featured-image {
            max-width: 100%;
        }
        
        img[src*="/books/"],
        img[src*="/courses/"],
        img[src*="/videos/"] {
            max-width: 100%;
            width: auto;
        }
    }
</style> 

    </main>

    <footer class="site-footer">
        <p>&copy; 2025 Commandz.io. All rights reserved.</p>
    </footer>

    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
</body>
</html> 